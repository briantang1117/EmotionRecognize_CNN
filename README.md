### 深度可分离卷积神经网络
-  本文将传统卷积网络与深度可分离卷积网络结合使用，网络结构如下图所示，在模型前部分先使用传统卷积，再让两者交替使用，可以充分发挥两者优势，达到更佳的效果。 
![网络结构](https://images.gitee.com/uploads/images/2021/0607/114129_5377d15c_8852279.png "网络结构.png")  
-  网络模型共含有19层，其中7层传统卷积层、8层深度可分离卷积层、4层最大池化层。同时，使用了 Adam优化器及对数损失函数。网络结构如图4所示，顺序从左至右、从上至下，并做以下说明： 
1. Conv为传统卷积层，其后3个参数分别代表：卷积核个数、卷积核大小、步长。
2. activation表示该层对应的激活函数。
3. SeparableConv为深度可分离卷积层，其后2个参数分别代表：卷积核个数、卷积核大小，步长均为 1。 
4. MaxPooing为最大池化层，其后2个参数分别代表：滤波器大小、步长。 
5. ReLU为线性整流函数，作为卷积后的激活函数，相比sigmoid函数和tanh函数有着更好的效果。 
6. softmax用于将最后一层卷积输出的七个数值映射到（0,1）区间，并使它们和为 1。 这样能更直观地以概率的形式显示结果。 
7. 在每一层卷积过后，都加入了批量归一化（Batch Normalization，BN）层，图中未标出。批量归一化对网络训练的各个方面都有一定的提升作用。它可以加快训练并提高性能、解决梯度消失的问题、规范权重、优化网络梯度流等，所以很有必要加入。 
8. 整个网络参数数量仅为75906个，其中可训练参数为74420个，相较传统网络大大减少。 
![网络结构2](https://images.gitee.com/uploads/images/2021/0607/114146_e097c4d1_8852279.png "网络结构2.png")
### 实验结果与分析
-  本文的实验在FER－2013数据集上进行，由于该数据集的样本都是从网上爬取且数量较多，对提升网络鲁棒性有较大帮助，比起 JAFFE、CK＋等数据库有更好的效果。
FER－2013数据集由35886张人脸不同表情图片组成，其中训练集28708张，验证集和测试集各3589张。每张图像都是分辨率为48*48的单通道灰度图像。表情分为7种：生气、厌恶、恐惧、开心、正常、伤心、惊讶。  
-  实验在macOS 11.0.1系统下进行，配置了2.8 GHz 四核Intel Core i7处理器，16GB内存，同时配置了Python 3.8、Tensorflow 2.3.1 框架。训练时，设置批次数量为32，训练次数预设为200次。在每一次迭代结束后，系统会记录相应的训练集损失值、训练集准确率、验证集损失值和验证集准确率。  
-  训练过程中验证集准确率和损失值变化如图6所示，此处一代（epoch）表示所有训练集数据训练一次，而非一个批次（batch），准确率从10次迭代后有较大提升，100次迭代后趋于稳定。验证集损失从最开始的1.8002下降到0.9750，准确率达到63.75％。